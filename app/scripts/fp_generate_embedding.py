import os
import json
from dotenv import load_dotenv
from app.services.ocr_extractor import extract_function_sentences_from_pdf
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, Document
from langchain_community.vectorstores import Qdrant
from langchain_community.embeddings import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from tqdm import tqdm
from app.utils.parser import postprocess_llm_output

load_dotenv()

DATA_DIR = "app/data/requirements"
PDF_FILES = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]

llm = ChatGoogleGenerativeAI(
    model= "gemini-2.5-flash-lite-preview-06-17",
    temperature=0.8,
    max_output_tokens=2048,
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

prompt_template = """
ì£¼ì˜: ë‹¹ì‹ ì€ ì ˆëŒ€ <thinking> ëª¨ë“œ ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. Thinking ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë˜ì—ˆìœ¼ë©° ì¦‰ì‹œ ê²°ê³¼ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ë¶„ì„ ë° ê¸°ëŠ¥ ì ìˆ˜í™”(Function Point Analysis) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ë¬¸ì¥ê³¼, ê´€ë ¨ ë¬¸ë§¥(ê·¼ê±°)ì…ë‹ˆë‹¤.
ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ëŠ¥ ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ê³ , ê° ê¸°ëŠ¥ì— ëŒ€í•´ IFPUG ê¸°ì¤€ì— ë”°ë¼ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ì¶”ë¡ í•´ì£¼ì„¸ìš”. 
í˜•ì‹ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ì‘ì„±í•´ì£¼ì„¸ìš”. 

[ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ë¬¸ì¥]
{question}

[ë¬¸ë§¥]
{context}

ì¶œë ¥ í˜•ì‹:
[
  {{
    "function_name": "ê¸°ëŠ¥ëª… (ê°„ê²°í•˜ê²Œ)",
    "description": "ê¸°ëŠ¥ ì„¤ëª…",
    "fp_type": "EI | EO | EQ | ILF | EIF ì¤‘ í•˜ë‚˜",
    "complexity": "SIMPLE | MEDIUM | COMPLEX ì¤‘ í•˜ë‚˜",
    "estimated_ftr": ì¶”ì • FTR ìˆ˜ (ì •ìˆ˜, ë°˜ë“œì‹œ ìˆ«ì),
    "estimated_det": ì¶”ì • DET ìˆ˜ (ì •ìˆ˜, ë°˜ë“œì‹œ ìˆ«ì),
  }}
]
 
"""

async def process_and_upload_to_qdrant():
    parsed_fp_results = []

    print(f"ğŸ“„ ì²˜ë¦¬í•  PDF íŒŒì¼ ëª©ë¡: {PDF_FILES}")

    for file in tqdm(PDF_FILES, desc="PDF ì²˜ë¦¬ ì¤‘"):
        print(f"â¡ï¸ í˜„ì¬ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file}")
        with open(os.path.join(DATA_DIR, file), "rb") as f:
            pdf_bytes = f.read()
        print(f"âœ… PDF íŒŒì¼ ì½ê¸° ì™„ë£Œ: {file}")

        sentences = await extract_function_sentences_from_pdf(pdf_bytes)
        print(f"âœ… ë¬¸ì¥ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(sentences)} ë¬¸ì¥")

        for idx, s in enumerate(tqdm(sentences, desc=f"â†’ {file} ë¬¸ì¥ ì²˜ë¦¬ ì¤‘", leave=False)):
            try:
                print(f"ğŸŸ£ ë¬¸ì¥ {idx+1}/{len(sentences)} ì²˜ë¦¬ ì¤‘: {s[:30]}...")

                user_input = prompt_template.format(question=s, context="")
                res = llm.invoke([HumanMessage(content=user_input)])
                print(f"âœ… LLM í˜¸ì¶œ ì™„ë£Œ")

                results = postprocess_llm_output(res.content)
                print(f"âœ… LLM ì‘ë‹µ íŒŒì‹± ì™„ë£Œ")

                if not results:
                    print(f"âš  JSON íŒŒì‹± ì‹¤íŒ¨: {s}\nì‘ë‹µ ì›ë¬¸:\n{res.content}")
                    continue

                json_obj = results[0]
                json_obj["original_sentence"] = s
                parsed_fp_results.append(json_obj)

            except Exception as e:
                print(f"âŒ ë¬¸ì¥ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                continue

    print(f"âœ… ì „ì²´ ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ, ì´ {len(parsed_fp_results)} ê±´ ê²°ê³¼ ì €ì¥ ì‹œì‘")

    os.makedirs("app/data", exist_ok=True)
    with open("app/data/fp_generated_dataset.json", "w", encoding="utf-8") as f:
        json.dump(parsed_fp_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ")

    documents = [
        Document(
            page_content=ex["description"],
            metadata={
                "function_name": ex["function_name"],
                "fp_type": ex["fp_type"],
                "complexity": ex["complexity"],
                "estimated_det": int(ex["estimated_det"]),
                "estimated_ftr": int(ex["estimated_ftr"]),
            }
        )
        for ex in parsed_fp_results
    ]
    print(f"âœ… Document ë³€í™˜ ì™„ë£Œ: ì´ {len(documents)}ê±´")

    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    print(f"âœ… Embedding ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    client = QdrantClient(host="localhost", port=6333)
    print(f"âœ… Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")

    client.recreate_collection(
        collection_name="fp_examples",
        vectors_config={"size": 384, "distance": "Cosine"}
    )
    print(f"âœ… Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")

    vectorstore = Qdrant(
        client=client,
        collection_name="fp_examples",
        embeddings=embedding
    )
    vectorstore.add_documents(documents)
    print(f"âœ… Qdrant ì—…ë¡œë“œ ì™„ë£Œ - ì´ {len(documents)}ê±´ ì™„ë£Œë¨")

